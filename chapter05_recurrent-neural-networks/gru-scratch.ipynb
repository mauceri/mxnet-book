{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Gated recurrent unit (GRU) RNNs\n",
    "\n",
    "This chapter requires some exposition. The GRU updates are fully implemented and the code appears to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "mx.random.seed(1)\n",
    "#ctx = mx.gpu(0)\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: \"The Time Machine\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/nlp/timemachine.txt\") as f:\n",
    "    time_machine = f.read()\n",
    "time_machine = time_machine[:-38083]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical representations of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_list = list(set(time_machine))\n",
    "vocab_size = len(character_list)\n",
    "character_dict = {}\n",
    "for e, char in enumerate(character_list):\n",
    "    character_dict[char] = e\n",
    "time_numerical = [character_dict[char] for char in time_machine]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hots(numerical_list, vocab_size=vocab_size):\n",
    "    result = nd.zeros((len(numerical_list), vocab_size), ctx=ctx)\n",
    "    for i, idx in enumerate(numerical_list):\n",
    "        result[i, idx] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textify(embedding):\n",
    "    result = \"\"\n",
    "    indices = nd.argmax(embedding, axis=1).asnumpy()\n",
    "    for idx in indices:\n",
    "        result += character_list[int(idx)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_length = 64\n",
    "# -1 here so we have enough characters for labels later\n",
    "num_samples = (len(time_numerical) - 1) // seq_length\n",
    "dataset = one_hots(time_numerical[:seq_length*num_samples]).reshape((num_samples, seq_length, vocab_size))\n",
    "num_batches = len(dataset) // batch_size\n",
    "train_data = dataset[:num_batches*batch_size].reshape((num_batches, batch_size, seq_length, vocab_size))\n",
    "# swap batch_size and seq_length axis to make later access easier\n",
    "train_data = nd.swapaxes(train_data, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = one_hots(time_numerical[1:seq_length*num_samples+1])\n",
    "train_label = labels.reshape((num_batches, batch_size, seq_length, vocab_size))\n",
    "train_label = nd.swapaxes(train_label, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated recurrent units (GRU) RNNs\n",
    "\n",
    "Similar to LSTM blocks, the GRU also has mechanisms to enable \"memorizing\" information for an extended number of time steps. However, it does so in a more expedient way:\n",
    "\n",
    "* We no longer keep a separate memory cell $c_t$. Instead, $h_{t-1}$ is added to a \"new content\" version of itself to give $h_{t}$.\n",
    "* The \"new content\" version is given by $g_t = \\text{tanh}(X_t W_{xh} + (r_t \\odot h_{t-1})W_{hh} + b_h ),$ and is analogous to $g_t$ in the LSTM tutorial.\n",
    "* Here, there is a reset gate $r_t$ which moderates the impact of $h_{t-1}$ on the \"new content\" version.\n",
    "* The input gate $i_t$ and forget gate $f_t$ are replaced by an single update gate $z_t$, which weighs the old and new content via $z_t$ and $(1 - z_t)$ respectively.\n",
    "* There is no output gate $o_t$; the weighted sum is what becomes $h_t$.\n",
    "\n",
    "We use the GRU block with the following transformations that map inputs to outputs across blocks at consecutive layers and consecutive time steps: $\\newcommand{\\xb}{\\mathbf{x}} \\newcommand{\\RR}{\\mathbb{R}}$\n",
    "\n",
    "$$z_t = \\sigma(X_t W_{xz} + h_{t-1} W_{hz} + b_z),$$\n",
    "\n",
    "$$r_t = \\sigma(X_t W_{xr} + h_{t-1} W_{hr} + b_r),$$\n",
    "\n",
    "$$g_t = \\text{tanh}(X_t W_{xh} + (r_t \\odot h_{t-1})W_{hh} + b_h ),$$\n",
    "\n",
    "$$h_t = z_t \\odot h_{t-1} + (1-z_t) \\odot g_t,$$\n",
    "\n",
    "where $\\sigma$ and $\\text{tanh}$ are as before in the LSTM case. \n",
    "\n",
    "Empirically, GRUs have similar performance to LSTMs, while requiring less parameters and forgoing an internal time state. Intuitively, GRUs have enough gates/state for long-term retention, but not too much, so that training and convergence remain fast and convex. See the work of Chung et al. [2014] (https://arxiv.org/abs/1412.3555)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = vocab_size\n",
    "num_hidden = 256\n",
    "num_outputs = vocab_size\n",
    "\n",
    "########################\n",
    "#  Weights connecting the inputs to the hidden layer\n",
    "########################\n",
    "Wxz = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
    "Wxr = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
    "Wxh = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
    "\n",
    "########################\n",
    "#  Recurrent weights connecting the hidden layer across time steps\n",
    "########################\n",
    "Whz = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
    "Whr = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
    "Whh = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
    "\n",
    "########################\n",
    "#  Bias vector for hidden layer\n",
    "########################\n",
    "bz = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
    "br = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
    "bh = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
    "\n",
    "########################\n",
    "# Weights to the output nodes\n",
    "########################\n",
    "Why = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .01\n",
    "by = nd.random_normal(shape=num_outputs, ctx=ctx) * .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [Wxz, Wxr, Wxh, Whz, Whr, Whh, bz, br, bh, Why, by]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y_linear, temperature=1.0):\n",
    "    lin = (y_linear-nd.max(y_linear)) / temperature\n",
    "    exp = nd.exp(lin)\n",
    "    partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_rnn(inputs, h, temperature=1.0):\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        z = nd.sigmoid(nd.dot(X, Wxz) + nd.dot(h, Whz) + bz)\n",
    "        r = nd.sigmoid(nd.dot(X, Wxr) + nd.dot(h, Whr) + br)\n",
    "        g = nd.tanh(nd.dot(X, Wxh) + nd.dot(r * h, Whh) + bh)\n",
    "        h = z * h + (1 - z) * g\n",
    "        \n",
    "        yhat_linear = nd.dot(h, Why) + by\n",
    "        yhat = softmax(yhat_linear, temperature=temperature) \n",
    "        outputs.append(yhat)\n",
    "    return (outputs, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging the loss over the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = nd.array([0.], ctx=ctx)\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text by sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(prefix, num_chars, temperature=1.0):\n",
    "    #####################################\n",
    "    # Initialize the string that we'll return to the supplied prefix\n",
    "    #####################################\n",
    "    string = prefix\n",
    "\n",
    "    #####################################\n",
    "    # Prepare the prefix as a sequence of one-hots for ingestion by RNN\n",
    "    #####################################\n",
    "    prefix_numerical = [character_dict[char] for char in prefix]\n",
    "    input_sequence = one_hots(prefix_numerical)\n",
    "    \n",
    "    #####################################\n",
    "    # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
    "    #####################################    \n",
    "    h = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
    "    c = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
    "\n",
    "    #####################################\n",
    "    # For num_chars iterations,\n",
    "    #     1) feed in the current input\n",
    "    #     2) sample next character from from output distribution\n",
    "    #     3) add sampled character to the decoded string\n",
    "    #     4) prepare the sampled character as a one_hot (to be the next input)\n",
    "    #####################################    \n",
    "    for i in range(num_chars):\n",
    "        outputs, h = gru_rnn(input_sequence, h, temperature=temperature)\n",
    "        choice = np.random.choice(vocab_size, p=outputs[-1][0].asnumpy())\n",
    "        string += character_list[choice]\n",
    "        input_sequence = one_hots([choice])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 3.6526421483244946\n",
      "The Time Ma te at te an a t at t t ta a t at t aa t ta t at te t te te te a ta t t te at te a t at t t t t te te t te a to a the aa t at te a at the t te t te o at a ta te t a ta to t ta t t t ta ta t at a t at h he t a ta o t t te t te t t aa t at t at te at t a at a t t a ate t a at t at te te at t at ate t a te t at a t at a te t t t te t t t a t t at t at t t t te an t at a at th t t te o at the t at at t te at t aa t ta te t a t t at a ta tt t at t at ae t th t te t t te a te te te t a t th t t aa t te t te te a the at t at t at te t at te t at t at t a t te at te te a t t t t a te te t aa at t at t th te t at a t te t te t to aa tae t t a t the a t aa t te t t a at t ta te t at at at t t at t ate t t th at a at t ta tt a ta ao th te te te th t ate te to t t at th a t te te te t t at ta at at te t t te t at t at at at at t a t th a te t at t a t at a tt a th at a tt at t an t a te a t atte t a t te t t at at t at t te at t te t te t te te t ta t t t aa t at th t aa te t at te t at t ta at a a t a t at te t at a t t\n",
      "The Medical Man rose, came to the lamp, a te t t ta a at te t at a te t h o t te t tot te oa t te te t te ta t t the t te at at t ta ta te t to aa t t ta t at at o t t aa t t t a at to te te at t te te te t at te a a t t a t to te t ta t at a t te a t te t te t at he a tt te t at a te t a to t at t t at te at at a t o t at at at t at te t at t an te te t ate t o ta t a t t t ae t at t at te t te ae t ate tt te te aa t a t ta t aa te t at te t ate t ta t t t th h an t th at te t at th to te te a ot ta t te te t te te te t a t a t t te t t t at te t a te at t te te at t t at at a t o t t o ta to ot t aa t o ta te t te t a a te t te t te a ta te a t ta t aa t at a te te te t h te t t te at a th t t at ta at at at h te t t a ta at t a ta a at t a t t t at t at a te t a te t t a t t t te t te a te th t t at the at te te a t he t t t th te t the t t te t aat te t t at an t the a t te a t at te t te te t ta te t at t t te t at a t te t at te t a te at te a t a ta to t at te a t te t ai t t at t te t ta t te t t t t a ot at a t t oa t t at te t te t ta at\n",
      "Epoch 10. Loss: 2.1327740044324806\n",
      "The Time Mas and the stored the stored the stored and the stored the same the stored the same the stored the stored the same the stored the same the stored the same the stored the stored the same the same the stored the stored the stored the same the same the sad the same the stored the stome the stored the stored the same the stored the stored the stored the same the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the same the stored the stored the same the stored the stored the sand the stored the stored the stored the stored the stored the stored the same the stored the same the stored the stored the same the stored the stored the stored the same the stored the same the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the same the stored the same the same the same the stored the same the stored the same the\n",
      "The Medical Man rose, came to the lamp, and the stored the same the sad the same the stored the stored the stored the same the stored the same the stored the same the same the stored the stored the stored the stored the stored the stored the same the same the stored the stored the stored the sand the stored the stored the stored the stored and the same the stored the stored the stored the stored the same the same the same the stored the stored the same the sad the stored the stored the stored the same the stored the same and the same the same the stored the same the stored the same the stored the sand the stored the stored the stored the same the same the stored the stored the same the stored the same the stored the same the stored the same the stored the stored the stored the same the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the stored the same the stored the stored the stored the stored the stored the stored the same the stored\n",
      "Epoch 20. Loss: 1.8488372732775462\n",
      "The Time Machine that the stard of the strange the strange the stard the stored the stard that the stard the stard of the stard of the strange the stard the still the stored the stored the strace of the stard of the starned the stored the starned the strange the still the stard the stard of the strace of the strange the stored the strange that the strace of the stard the stard of the stard the starned the stard the stored the stored the stored the stard the stored the started the stored the same that the story of the stard the strace of the strange that the strace that the stard the stard and the stards the starned the stard of the stard of the stard the stard that the stored the strace of the story of the strange the stard and the stard the stard of the starned the strange the still the stard the stard of the stards the stored the strack the stard the stard the still the stards the stard and the stard the stored the startly strange the stard the stard the stard and the stored the strange the stored the still the stard \n",
      "The Medical Man rose, came to the lamp, and the stard the stored the strange the stard the strace of the stard the stard of the strange the stards the stored the starned the still the strace of the strange the stard the stards the strace of the stard of the stard and the starned the strange the stored the stard the stards the stored the stard the stored the strang the stard of the stard the stored the stored the stard of the stard the stards the still the stard the stard the stard the still the strange that the stored the stard the strace of the stored the stard the story of the stard the stored the strange the still the stards the strace of the story the still the stard and the stard of the stard the strange that the strought the stard of the starned the starded the stards the stard of the stard of the stard the starned the stards the stard of the stard of the stard of the stard, and the stard of the strace of the still the starned the stard the stored the strange that the stored the stard that the stards and the stored the story the stored the s\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "moving_loss = 0.\n",
    "\n",
    "learning_rate = 2.0\n",
    "\n",
    "# state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "for e in range(epochs):\n",
    "    ############################\n",
    "    # Attenuate the learning rate by a factor of 2 every 100 epochs.\n",
    "    ############################\n",
    "    if ((e+1) % 100 == 0):\n",
    "        learning_rate = learning_rate / 2.0\n",
    "    h = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "    for i in range(num_batches):\n",
    "        data_one_hot = train_data[i]\n",
    "        label_one_hot = train_label[i]\n",
    "        with autograd.record():\n",
    "            outputs, h = gru_rnn(data_one_hot, h)\n",
    "            loss = average_ce_loss(outputs, label_one_hot)\n",
    "            loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if (i == 0) and (e == 0):\n",
    "            moving_loss = nd.mean(loss).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
    "    if e%10==0:\n",
    "        print(\"Epoch %s. Loss: %s\" % (e, moving_loss)) \n",
    "        print(sample(\"The Time Ma\", 1024, temperature=.1))\n",
    "        print(sample(\"The Medical Man rose, came to the lamp,\", 1024, temperature=.1))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "[Placeholder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[Simple, LSTM, and GRU RNNs with gluon](../chapter05_recurrent-neural-networks/rnns-gluon.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
